---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
---

Iâ€™m interested in how individuals, groups, and AI systems might interact with each other, and the \{statistical, algorithmic, ...\} questions that such interactions raise. I think of theory as (i) a means to provide tools for addressing such problems, and (ii) a language for understanding these ideas at a conceptual level. Iâ€™m especially excited when technical work can be informed by perspectives from sociology, political theory, philosophy, and other humanistic disciplines. 

_much love to everyone I've gotten to work with_ ðŸ™‚

### Manuscripts

- Jessica Dai, Nika Haghtalab, Eric Zhao _(alphabetical order)_. **Learning With Multi-Group Guarantees For Clusterable Subpopulations**{: .emph}. [[PDF]](https://arxiv.org/abs/2410.14588v1){:target="_blank"}

### Conference papers

- Jessica Dai. **Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis**{: .emph}. _Position Paper,_ ICML 2024 (**oral**). [[PDF]](https://arxiv.org/abs/2404.13861){:target="_blank"} [[Poster]](https://www.jessicad.ai/pdfs/agency_poster.pdf){:target="_blank"} [[Talk]](https://icml.cc/virtual/2024/oral/35574){:target="_blank"}
- Jessica Dai, Bailey Flanigan, Nika Haghtalab, Meena Jagadeesan, Chara Podimata _(alphabetical order)_. **Can Probabilistic Feedback Drive User Impacts in Online Platforms?**{: .emph} AISTATS 2024. [[PDF]](https://arxiv.org/abs/2401.05304){:target="_blank"} [[Poster]](https://www.jessicad.ai/pdfs/probfeedback_poster.pdf){:target="_blank"}
- Jessica Dai, Paula Gradu, Chris Harshaw _(alphabetical order)_. **Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments.**{: .emph} NeurIPS 2023 (**spotlight**). [[PDF]](https://arxiv.org/abs/2305.17187){:target="_blank"} [[INFORMS Slides]](https://www.jessicad.ai/pdfs/clip-ogd__informs.pdf){:target="_blank"}
- Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H. Bach, Himabindu Lakkaraju. **Fairness via Explanation Quality:
Evaluating Disparities in the Quality of Post hoc Explanations.**{: .emph} AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) 2022. [[PDF]](https://arxiv.org/abs/2205.07277){:target="_blank"} 
- Jessica Dai, Sina Fazelpour, Zachary C. Lipton. **Fair Machine Learning Under Partial Compliance.**{: .emph} AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) 2021 (**oral**). Previously in Workshop on Consequential Decisions in Dynamic Environments at NeurIPS 2020 (**oral**); Workshop on Machine Learning for Economic Policy at NeurIPS 2020;
Women in Machine Learning Workshop at NeurIPS 2020 (**oral**). [[PDF]](https://arxiv.org/abs/2011.03654){:target="_blank"} [[Video]](https://slideslive.com/38942278/fair-machine-learning-under-partial-compliance)


### Workshop papers

- Jessica Dai and Eve Fleisig _(alphabetical order)_. **Mapping Social Choice Theory to RLHF**{: .emph}. Workshop on Reliable and Responsible Foundation Models at ICLR 2024. [[PDF]](https://arxiv.org/abs/2404.13038){:target="_blank"}
- Kweku Kwegyir-Aggrey, A. Feder Cooper, Jessica Dai, John P. Dickerson, Keegan Hines, Suresh Venkatasubramanian. **Repairing Regressors for Fair Binary Classification at Any Decision Threshold.**{: .emph} Workshop on Algorithmic Fairness through the Lens of Time at NeurIPS 2023 (**oral**; PMLR proceedings). [[PDF]](https://arxiv.org/abs/2203.07490){:target="_blank"}
- Jessica Dai, Sohini Upadhyay, Stephen H. Bach, Himabindu Lakkaraju. **What will it take to generate fairness-preserving explanations?**{: .emph} ICML Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI 2021. [[PDF]](https://arxiv.org/pdf/2106.13346.pdf){:target="_blank"} [[Poster]](https://github.com/ICML2021-XAI/Summary-Slides/blob/main/What%20will%20it%20take%20to%20generate%20fairness-preserving%20explanations.pdf)
- Jessica Dai and Sarah M. Brown. **Label Bias, Label Shift: Fair Machine Learning with Unreliable Labels.**{: .emph} Workshop on Consequential Decisions in Dynamic Environments at NeurIPS 2020; Women in Machine Learning Workshop at NeurIPS 2020. [[PDF]](https://dynamicdecisions.github.io/assets/pdfs/29.pdf)
